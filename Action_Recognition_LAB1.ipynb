{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZYrzF86Euot"
      },
      "source": [
        "# Action recognition course : LAB 1\n",
        "\n",
        "> Author : BABIN-RIBY Hugo, See LICENSE FIle\n",
        "\n",
        "This lab is meant to be completed by students. There is a set of challenges they need to overcome by asking questions to gain experience.\n",
        "\n",
        "> Note that this notebook was meant to be executed in google colab but can easily be ported to other services with minimal work.\n",
        "\n",
        "At the end of this notebook (~1h30-2h) you will :\n",
        "\n",
        "- Have a better expertise in video data inference\n",
        "- Have hands-on experience with\n",
        "  - action recognition models\n",
        "  - action recognition datasets\n",
        "- Know the pros and cons of each type of architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTxZ92YUFCAi"
      },
      "source": [
        "## 1 : Gathering data\n",
        "\n",
        "Before doing anything fancy, we need some video data. For this first lab, w'ell focus on the **kinetics 400** dataset to compare networks execution time and precisions.\n",
        "\n",
        "Gathering data and pre-processing it is often a very challenging part in industry projects. In this lab, the data will be provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxq3_BZUAdbl"
      },
      "outputs": [],
      "source": [
        "# Deps\n",
        "%pip install torch\n",
        "import torch\n",
        "import urllib\n",
        "import platform\n",
        "%pip install -q yt-dlp pandas tqdm &> /dev/null\n",
        "%pip install pytorchvideo &> /dev/null\n",
        "%pip install --upgrade pytorchvideo &> /dev/null\n",
        "%apt update &> /dev/null\n",
        "%pip install torchvision\n",
        "# Kinetics label file : Kinetics has a lot of classes (400), this file associate each ID with a class name\n",
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)\n",
        "# test by importing a model form pytorchvideo\n",
        "test = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKrU4OM5G_nv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download kinetics 400 videos\n",
        "import pandas as pd\n",
        "import os\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "def download_kinetics_samples(num_samples=20, output_dir='kinetics_samples'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"Created directory: {output_dir}\")\n",
        "\n",
        "    metadata_file = \"kinetics400/train.csv\"\n",
        "    if not os.path.exists(metadata_file):\n",
        "        print(\"Downloading Kinetics metadata...\")\n",
        "        url = \"https://storage.googleapis.com/deepmind-media/Datasets/kinetics400.tar.gz\"\n",
        "        urllib.request.urlretrieve(url, \"kinetics400.tar.gz\")\n",
        "\n",
        "        print(\"Extracting metadata...\")\n",
        "        with tarfile.open(\"kinetics400.tar.gz\", \"r:gz\") as tar:\n",
        "            tar.extractall()\n",
        "        os.remove(\"kinetics400.tar.gz\")\n",
        "\n",
        "    print(\"Reading metadata...\")\n",
        "    df = pd.read_csv(metadata_file)\n",
        "\n",
        "    samples = df.sample(n=num_samples, random_state=42)\n",
        "    print(f\"Selected {num_samples} random samples\")\n",
        "\n",
        "    successful_downloads = 0\n",
        "    for idx, row in tqdm(samples.iterrows(), total=num_samples, desc=\"Downloading videos\"):\n",
        "        video_id = row['youtube_id']\n",
        "        start_time = row['time_start']\n",
        "        end_time = row['time_end']\n",
        "        label = row['label'].replace(' ', '_')\n",
        "\n",
        "        output_path = os.path.join(output_dir, f\"{label}-{video_id}-{start_time}-{end_time}.mp4\")\n",
        "\n",
        "        if not os.path.exists(output_path):\n",
        "            try:\n",
        "                command = [\n",
        "                    'yt-dlp',\n",
        "                    f'https://youtube.com/watch?v={video_id}',\n",
        "                    '--quiet',\n",
        "                    '--format', 'mp4',\n",
        "                    '--output', output_path,\n",
        "                    '--postprocessor-args',\n",
        "                    f'-ss {start_time} -t {end_time-start_time}'\n",
        "                ]\n",
        "                result = subprocess.run(command, capture_output=True, text=True)\n",
        "\n",
        "                if os.path.exists(output_path):\n",
        "                    successful_downloads += 1\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError downloading {video_id}: {str(e)}\")\n",
        "\n",
        "    print(f\"Download complete to {output_dir}/\")\n",
        "    return successful_downloads\n",
        "\n",
        "# this will later be used for testing\n",
        "N = download_kinetics_samples(200, output_dir='kinetics_samples') - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqXgWwMlIgQr"
      },
      "source": [
        "### Prepare the data for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeN_APR3Ifpm"
      },
      "outputs": [],
      "source": [
        "# Lets visualize the data..\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IKR1sE2CRU4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as pl\n",
        "\n",
        "video_dir = \"kinetics_samples\"\n",
        "\n",
        "videos = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
        "\n",
        "if not videos:\n",
        "    raise Exception(f\"No videos found in {video_dir}\")\n",
        "\n",
        "# Select random video\n",
        "video_name = random.choice(videos)\n",
        "video_path = os.path.join(video_dir, video_name)\n",
        "\n",
        "print(f\"Selected video: {video_name}\")\n",
        "\n",
        "# Read the video\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "video_data = video.get_clip(start_sec=0, end_sec=1)\n",
        "video_data = video_data[\"video\"]\n",
        "\n",
        "# imshow but in rgb\n",
        "print(video_data.shape)\n",
        "frame = video_data[:, 0, :, :].permute(1, 2, 0)\n",
        "print(frame.shape)\n",
        "plt.imshow(frame/255)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKDRxRP8O2Of"
      },
      "outputs": [],
      "source": [
        "# Create DATALOADERS builders for different models\n",
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "# CREATE ID <> CLASS NAME TRANSLATORS\n",
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
        "\n",
        "classname_to_kinetics_id = {v: k for k, v in kinetics_id_to_classname.items()}\n",
        "\n",
        "\n",
        "def get_transform(side_size, crop_size, num_frames) :\n",
        "  \"\"\"Returns a transform to pre-process video frames dpending on models specificities.\"\"\"\n",
        "  mean = [0.45, 0.45, 0.45]\n",
        "  std = [0.225, 0.225, 0.225]\n",
        "  transform =  ApplyTransformToKey(\n",
        "      key=\"video\",\n",
        "      transform=Compose(\n",
        "          [\n",
        "              UniformTemporalSubsample(num_frames),\n",
        "              Lambda(lambda x: x/255.0),\n",
        "              NormalizeVideo(mean, std),\n",
        "              ShortSideScale(\n",
        "                  size=side_size\n",
        "              ),\n",
        "              CenterCropVideo(crop_size)\n",
        "          ]\n",
        "      ),\n",
        "  )\n",
        "  return transform\n",
        "\n",
        "class VideoDatasetBuilder(Dataset):\n",
        "    def __init__(self, video_folder, transform, clip_duration):\n",
        "        self.video_folder = video_folder\n",
        "        self.transform = transform\n",
        "        self.clip_duration = clip_duration\n",
        "        self.file_list = []\n",
        "        for filename in os.listdir(video_folder):\n",
        "          if os.path.isfile(os.path.join(video_folder, filename)):\n",
        "            self.file_list.append(filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get video path and label a,d start timastamp\n",
        "        video_path = self.file_list[idx]\n",
        "        class_name = video_path.split('-')[0]\n",
        "        class_name = class_name.replace(\"_\", \" \")\n",
        "        class_id = classname_to_kinetics_id.get(class_name, None)\n",
        "        label = [0] * 400\n",
        "        label[class_id] = 1\n",
        "        label = torch.tensor(numpy.array(label))\n",
        "\n",
        "        start_sec = int(video_path.split('-')[-2])\n",
        "\n",
        "        # Load video\n",
        "        video = EncodedVideo.from_path(\"./kinetics_samples/\"+video_path)\n",
        "        video_data = video.get_clip(start_sec=start_sec, end_sec=start_sec+self.clip_duration)\n",
        "        video_data = self.transform(video_data)['video']\n",
        "\n",
        "\n",
        "        return video_data, label\n",
        "\n",
        "\n",
        "# Test the data set builder\n",
        "test_data_loader = VideoDatasetBuilder(video_folder=\"./\"+video_dir, transform=get_transform(256,256,8), clip_duration=9)\n",
        "test_sample = test_data_loader[5]\n",
        "print(test_sample[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIbRRfwIUut"
      },
      "source": [
        "## 2 : Using Slowfast\n",
        "\n",
        "First we will use the SlowFast model :\n",
        "\n",
        "1. Import & declare the pre-trained model\n",
        "2. Run inference\n",
        "3. Gather the following metrics\n",
        "  - Accuracy\n",
        "  - Execution time over 200 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PYyoTE9JR2n"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "slowfast_model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "slowfast_model = slowfast_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgP-5WgCIN7w"
      },
      "source": [
        "### TEST #1 : basic archery data\n",
        "\n",
        "inpired from : https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO35cdvkII7r"
      },
      "outputs": [],
      "source": [
        "# get archery video\n",
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)\n",
        "clip_duration = 9\n",
        "\n",
        "# CROP VIDEO FOR SLOWFAST\n",
        "\n",
        "# Slowfasts's specific input metadata\n",
        "side_size = 256\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "num_clips = 10\n",
        "num_crops = 3\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# DECLARE SLOWFAST SPECIFIC TRASNFORMATIONS\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "sf_transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "video_data = sf_transform(video_data)\n",
        "\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = [i.to(device)[None, ...] for i in inputs]\n",
        "\n",
        "preds= slowfast_model(inputs)\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))\n",
        "\n",
        "# Check out what is going into the model\n",
        "inputs = inputs[0]\n",
        "print(inputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1zUL9x5PJ3b"
      },
      "source": [
        "### Test on actual kinetics400 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eiBAU3qPJRy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Lists to store results\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# declare dataset based on kinetics 400\n",
        "tsm_dataset = VideoDatasetBuilder(video_folder=\"./\"+video_dir, transform=sf_transform, clip_duration=clip_duration)\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(N):\n",
        "    sample = tsm_dataset[i]\n",
        "    inputs = sample[0]\n",
        "    true_labels = sample[1]\n",
        "    inputs = [i.to(device)[None, ...] for i in inputs]\n",
        "\n",
        "    # Get predictions\n",
        "    preds = slowfast_model(inputs)\n",
        "    post_act = torch.nn.Softmax(dim=1)\n",
        "    preds = post_act(preds)\n",
        "\n",
        "    # Get top 5 predictions\n",
        "    top5_preds = preds.topk(k=5)\n",
        "    pred_classes = top5_preds.indices[0]\n",
        "\n",
        "    # Convert predicted classes to label names\n",
        "    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "\n",
        "    # Find the true class (index where label is 1)\n",
        "    true_class_idx = torch.where(torch.tensor(true_labels) == 1)[0].item()\n",
        "    true_class_name = kinetics_id_to_classname[true_class_idx]\n",
        "\n",
        "    print(f\"\\nSample {i}:\")\n",
        "    print(f\"True label: {true_class_name}\")\n",
        "    print(f\"Top 5 predictions: {', '.join(pred_class_names)}\")\n",
        "\n",
        "    # Store results for accuracy computation\n",
        "    all_predictions.append(pred_classes.cpu())\n",
        "    all_true_labels.append(true_class_idx)\n",
        "\n",
        "end_time=time.time()\n",
        "\n",
        "# Convert to tensors for easier computation\n",
        "all_predictions = torch.stack(all_predictions)\n",
        "all_true_labels = torch.tensor(all_true_labels)\n",
        "\n",
        "# Compute Top-1 accuracy\n",
        "top1_correct = (all_predictions[:, 0] == all_true_labels).sum().item()\n",
        "top1_accuracy = (top1_correct / N) * 100\n",
        "\n",
        "# Compute Top-5 accuracy\n",
        "top5_correct = sum(true_label in pred_classes for true_label, pred_classes in zip(all_true_labels, all_predictions))\n",
        "top5_accuracy = (top5_correct / N) * 100\n",
        "\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
        "print(f\"Execution time =\", round(end_time - start_time,2))\n",
        "print(f\"Execution time per sample =\", round((end_time - start_time)/N,2))\n",
        "print(f\"Sample duration =\", round(clip_duration,2))\n",
        "print(f\"model I/O rtio\" , round((end_time - start_time)/N/clip_duration,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1LhesoIJYid"
      },
      "source": [
        "## 3 : Using TSM\n",
        "\n",
        "First we will use the **TSM** model :\n",
        "\n",
        "1. Import & declare the pre-trained model\n",
        "2. Run inference\n",
        "3. Gather the following metrics\n",
        "  - Accuracy\n",
        "  - Execution time over 200 samples\n",
        "4. As a bonus, you can also try the more precise TSM versions. See TSM repo for pretrained links :\n",
        "  - https://github.com/mit-han-lab/temporal-shift-module?tab=readme-ov-file#kinetics-400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jomx1ffIV2F"
      },
      "source": [
        "### Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RG9HOfAJa9V"
      },
      "outputs": [],
      "source": [
        "!rm -rf temporal-shift-module\n",
        "!git clone https://github.com/mit-han-lab/temporal-shift-module.git\n",
        "%cd temporal-shift-module\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "from ops.models import TSN\n",
        "%cd ..\n",
        "# Get the weights, see TSM's github page. WATCH OUT ! all models does not yield same performance and does not ask for\n",
        "# the same method for pre-processing the data !\n",
        "!wget https://hanlab18.mit.edu/projects/tsm/models/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8tlj7dlOcts"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def create_tsm_model(weights_path, num_class=400):\n",
        "    # Model parameters\n",
        "    num_segments = 8\n",
        "    modality = 'RGB'\n",
        "    base_model = 'resnet50'\n",
        "    consensus_type = 'avg'\n",
        "    dropout = 0.8\n",
        "    img_feature_dim = 256\n",
        "\n",
        "    # TSM specific parameters\n",
        "    shift_div = 8  # Number of divisions for shift\n",
        "    shift_place = 'blockres'  # Where to place shift operations\n",
        "\n",
        "    # Initialize model\n",
        "    model = TSN(num_class=num_class,\n",
        "                num_segments=num_segments,\n",
        "                modality=modality,\n",
        "                base_model=base_model,\n",
        "                consensus_type=consensus_type,\n",
        "                dropout=dropout,\n",
        "                img_feature_dim=img_feature_dim,\n",
        "                partial_bn=False,\n",
        "                is_shift=True,  # This makes it TSM\n",
        "                shift_div=shift_div,\n",
        "                shift_place=shift_place).to(\"cpu\")\n",
        "\n",
        "    checkpoint = torch.load(weights_path)\n",
        "    state_dict = checkpoint['state_dict']\n",
        "\n",
        "    # Remove 'module.' prefix if it exists\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        # Remove 'module.' prefix if exists\n",
        "        if k.startswith('module.'):\n",
        "            k = k[7:]\n",
        "\n",
        "        # Fix the layer structure naming\n",
        "        k = k.replace('.block.', '.')\n",
        "        if '.nl.' in k:  # Skip non-local layers if they cause issues\n",
        "            continue\n",
        "\n",
        "        new_state_dict[k] = v\n",
        "\n",
        "    # Load the weights\n",
        "    model.load_state_dict(new_state_dict)\n",
        "\n",
        "    # Set to evaluation mode and move to GPU if available\n",
        "    model.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "weights_path = \"TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth\"\n",
        "if os.path.exists(weights_path) == False:\n",
        "  !wget https://hanlab18.mit.edu/projects/tsm/models/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e100_dense_nl.pth\n",
        "  !wget https://hanlab18.mit.edu/projects/tsm/models/TSM_kinetics_RGB_resnet50_shift8_blockres_avg_segment8_e50.pth\n",
        "  ...\n",
        "tsm_model = create_tsm_model(weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di4akGJfkFTK"
      },
      "outputs": [],
      "source": [
        "print(tsm_model.input_size)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "tsm_model = tsm_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrCNnRPAnHaV"
      },
      "source": [
        "### Test #1 for TSM on example \"archery\" data\n",
        "\n",
        "https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubn3fhy6nD4D"
      },
      "outputs": [],
      "source": [
        "# GET THE DATA\n",
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)\n",
        "# clip_duration = 9\n",
        "\n",
        "# CROP VIDEO FOR TSM\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "tsm_transform = get_transform(224,224,8)\n",
        "video_data = tsm_transform(video_data)\n",
        "\n",
        "inputs = video_data[\"video\"].to(device)\n",
        "preds= tsm_model(inputs)\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))\n",
        "\n",
        "# Check out what is going into the model\n",
        "print(inputs.shape)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(10, 8))\n",
        "for i in range(8):\n",
        "    col, row = divmod(i, 2)\n",
        "    axes[row, col].imshow(inputs[0, i, :, :].cpu(), cmap=\"gray\")\n",
        "    axes[row, col].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEBh9l84rM1H"
      },
      "source": [
        "### Testing on actual kinetics400 data...\n",
        "\n",
        "Let's try it on more elaborate data ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZsIxaTzAFuw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Lists to store results\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "start_time = time.time()\n",
        "# The clip duration metric is up to the appreciation and context (kinetics clips are 10 for exmaple)\n",
        "# We can also optimise this metric but it will not be covered in this course\n",
        "tsm_clip_duration = 0.6\n",
        "\n",
        "# declare dataset based on kinetics 400\n",
        "tsm_dataset = VideoDatasetBuilder(video_folder=\"./\"+video_dir, transform=get_transform(256,256,8), clip_duration=tsm_clip_duration)\n",
        "\n",
        "for i in range(N):\n",
        "    sample = tsm_dataset[i]\n",
        "    input = sample[0].to(device)\n",
        "    true_labels = sample[1]\n",
        "\n",
        "    # Get predictions\n",
        "    preds = tsm_model(input)\n",
        "    post_act = torch.nn.Softmax(dim=1)\n",
        "    preds = post_act(preds)\n",
        "\n",
        "    # Get top 5 predictions\n",
        "    top5_preds = preds.topk(k=5)\n",
        "    pred_classes = top5_preds.indices[0]\n",
        "\n",
        "    # Convert predicted classes to label names\n",
        "    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "\n",
        "    # Find the true class (index where label is 1)\n",
        "    true_class_idx = torch.where(torch.tensor(true_labels) == 1)[0].item()\n",
        "    true_class_name = kinetics_id_to_classname[true_class_idx]\n",
        "\n",
        "    print(f\"\\nSample {i}:\")\n",
        "    print(f\"True label: {true_class_name}\")\n",
        "    print(f\"Top 5 predictions: {', '.join(pred_class_names)}\")\n",
        "\n",
        "    # Store results for accuracy computation\n",
        "    all_predictions.append(pred_classes.cpu())\n",
        "    all_true_labels.append(true_class_idx)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Convert to tensors for easier computation\n",
        "all_predictions = torch.stack(all_predictions)\n",
        "all_true_labels = torch.tensor(all_true_labels)\n",
        "\n",
        "# Compute Top-1 accuracy\n",
        "top1_correct = (all_predictions[:, 0] == all_true_labels).sum().item()\n",
        "top1_accuracy = (top1_correct / N) * 100\n",
        "\n",
        "# Compute Top-5 accuracy\n",
        "top5_correct = sum(true_label in pred_classes for true_label, pred_classes in zip(all_true_labels, all_predictions))\n",
        "top5_accuracy = (top5_correct / N) * 100\n",
        "\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
        "print(f\"Execution time =\", round(end_time - start_time,2))\n",
        "print(f\"Execution time per sample =\", round((end_time - start_time)/N,2))\n",
        "print(f\"Sample duration =\", round(tsm_clip_duration,2))\n",
        "print(f\"model I/O rtio\" , round((end_time - start_time)/N/tsm_clip_duration,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sz0VeMlHayi"
      },
      "source": [
        "### Conclusion on TSM\n",
        "\n",
        "#### 1 : Precision results\n",
        "\n",
        "As we can see, the TSM usage on real data did not go as expected. Maybe this is due to data preprocessing not being exactly the same as the one proposed in the researcher's work (which is rather complex).\n",
        "\n",
        "Solution ? Using a fine-tuning layer would be a great option !\n",
        "\n",
        "#### 2 : Computing power\n",
        "\n",
        "THis model has a better I/O ratio than slowfast\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwpLp7duJdJO"
      },
      "source": [
        "## 4 : Using TSN\n",
        "\n",
        "Finally, we will test the TSN model :\n",
        "\n",
        "1. Import & declare the pre-trained model\n",
        "2. Run inference\n",
        "3. Gather the following metrics\n",
        "  - Accuracy\n",
        "  - Execution time over N samples\n",
        "\n",
        "TSN & TSM share the same base, We'll use the same methodology as TSM except we won't add any shifting, effectively creating a \"native\" TSN model.\n",
        "\n",
        "Resource : https://github.com/mit-han-lab/temporal-shift-module?tab=readme-ov-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EtrFhDNJiux"
      },
      "outputs": [],
      "source": [
        "# get the TSN weights\n",
        "!wget https://hanlab18.mit.edu/projects/tsm/models/TSM_kinetics_RGB_resnet50_avg_segment5_e50.pth\n",
        "\n",
        "# create the model\n",
        "from collections import OrderedDict\n",
        "\n",
        "def create_tsm_model(weights_path, num_class=400):\n",
        "    # Model parameters\n",
        "    num_segments = 8\n",
        "    modality = 'RGB'\n",
        "    base_model = 'resnet50'\n",
        "    consensus_type = 'avg'\n",
        "    dropout = 0.8\n",
        "    img_feature_dim = 256\n",
        "\n",
        "    # Initialize model & setting \"is_shift\" to False\n",
        "    model = TSN(num_class=num_class,\n",
        "                num_segments=num_segments,\n",
        "                modality=modality,\n",
        "                base_model=base_model,\n",
        "                consensus_type=consensus_type,\n",
        "                dropout=dropout,\n",
        "                img_feature_dim=img_feature_dim,\n",
        "                partial_bn=False,\n",
        "                is_shift=False,).to(\"cpu\")\n",
        "\n",
        "    checkpoint = torch.load(weights_path)\n",
        "    state_dict = checkpoint['state_dict']\n",
        "\n",
        "    # Remove 'module.' prefix if it exists\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        # Remove 'module.' prefix if exists\n",
        "        if k.startswith('module.'):\n",
        "            k = k[7:]\n",
        "\n",
        "        # Fix the layer structure naming\n",
        "        k = k.replace('.block.', '.')\n",
        "        if '.nl.' in k:  # Skip non-local layers if they cause issues\n",
        "            continue\n",
        "\n",
        "        new_state_dict[k] = v\n",
        "\n",
        "    # Load the weights\n",
        "    model.load_state_dict(new_state_dict)\n",
        "\n",
        "    # Set to evaluation mode and move to GPU if available\n",
        "    model.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "weights_path = \"TSM_kinetics_RGB_resnet50_avg_segment5_e50.pth\"\n",
        "tsn_model = create_tsm_model(weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCtfi00bu5tc"
      },
      "source": [
        "## Test on basic archery data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utKeva_uukyG"
      },
      "outputs": [],
      "source": [
        "# GET THE DATA\n",
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "if not os.path.exists(video_path):\n",
        "  try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "  except: urllib.request.urlretrieve(url_link, video_path)\n",
        "# clip_duration = 10\n",
        "\n",
        "# CROP VIDEO FOR TSN\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "tsm_transform = get_transform(224,224,8)\n",
        "video_data = tsm_transform(video_data)\n",
        "\n",
        "inputs = video_data[\"video\"].to(device)\n",
        "preds= tsn_model(inputs)\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))\n",
        "\n",
        "# Check out what is going into the model\n",
        "print(inputs.shape)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(10, 8))\n",
        "for i in range(8):\n",
        "    col, row = divmod(i, 2)\n",
        "    axes[row, col].imshow(inputs[0, i, :, :].cpu(), cmap=\"gray\")\n",
        "    axes[row, col].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N7XGt3dvp7T"
      },
      "source": [
        "## Test on actual kinetics400 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H78MlZJzvmfa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Lists to store results\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "start_time = time.time()\n",
        "# clip_duration = 10\n",
        "\n",
        "# declare dataset based on kinetics 400\n",
        "tsn_dataset = VideoDatasetBuilder(video_folder=\"./\"+video_dir, transform=get_transform(256,256,8), clip_duration=clip_duration)\n",
        "\n",
        "for i in range(N):\n",
        "    sample = tsn_dataset[i]\n",
        "    input = sample[0].to(device)\n",
        "    true_labels = sample[1]\n",
        "\n",
        "    # Get predictions\n",
        "    preds = tsn_model(input)\n",
        "    post_act = torch.nn.Softmax(dim=1)\n",
        "    preds = post_act(preds)\n",
        "\n",
        "    # Get top 5 predictions\n",
        "    top5_preds = preds.topk(k=5)\n",
        "    pred_classes = top5_preds.indices[0]\n",
        "\n",
        "    # Convert predicted classes to label names\n",
        "    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "\n",
        "    # Find the true class (index where label is 1)\n",
        "    true_class_idx = torch.where(torch.tensor(true_labels) == 1)[0].item()\n",
        "    true_class_name = kinetics_id_to_classname[true_class_idx]\n",
        "\n",
        "    print(f\"\\nSample {i}:\")\n",
        "    print(f\"True label: {true_class_name}\")\n",
        "    print(f\"Top 5 predictions: {', '.join(pred_class_names)}\")\n",
        "\n",
        "    # Store results for accuracy computation\n",
        "    all_predictions.append(pred_classes.cpu())\n",
        "    all_true_labels.append(true_class_idx)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Convert to tensors for easier computation\n",
        "all_predictions = torch.stack(all_predictions)\n",
        "all_true_labels = torch.tensor(all_true_labels)\n",
        "\n",
        "# Compute Top-1 accuracy\n",
        "top1_correct = (all_predictions[:, 0] == all_true_labels).sum().item()\n",
        "top1_accuracy = (top1_correct / N) * 100\n",
        "\n",
        "# Compute Top-5 accuracy\n",
        "top5_correct = sum(true_label in pred_classes for true_label, pred_classes in zip(all_true_labels, all_predictions))\n",
        "top5_accuracy = (top5_correct / N) * 100\n",
        "\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
        "print(f\"Execution time =\", round(end_time - start_time,2))\n",
        "print(f\"Execution time per sample =\", round((end_time - start_time)/N,2))\n",
        "print(f\"Sample duration =\", round(clip_duration,2))\n",
        "print(f\"model I/O rtio\" , round((end_time - start_time)/N/clip_duration,2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
