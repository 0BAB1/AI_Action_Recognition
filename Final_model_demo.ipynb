{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL DEMO\n",
    "\n",
    "Demo of the optimized slofast model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 : Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "slowfast = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "\n",
    "# Setup fine tuning layer\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes=2):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Import weights\n",
    "fine_tune = CustomClassifier(400, [512, 128], 1)\n",
    "weights_path = \"./fine_tune_400_512-128_1.pth\"\n",
    "fine_tune.load_state_dict(torch.load(weights_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a master model\n",
    "\n",
    "# Setup fine tuning layer\n",
    "class CrimeDetector(nn.Module):\n",
    "    def __init__(self, slowfast, fine_tuned):\n",
    "        super(CrimeDetector, self).__init__()\n",
    "        self.slowfast = slowfast\n",
    "        self.fine_tuned = fine_tuned\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.slowfast(x)\n",
    "        return self.fine_tuned(x)\n",
    "    \n",
    "crime_detector = CrimeDetector(slowfast, fine_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the data\n",
    "\n",
    "To setup data, we need a few things :\n",
    "\n",
    "- A transform to scale and crop our video (pytorch video utils)\n",
    "- A transform to pack data for slowfast double channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "slowfast_alpha = 4\n",
    "num_clips = 10\n",
    "num_crops = 3\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# !ls\n",
    "\n",
    "video = EncodedVideo.from_path(\"./DCSASS/Shoplifting/Shoplifting001_x264.mp4/Shoplifting001_x264_19.mp4\")\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "video_data = transform(video_data)\n",
    "inputs = video_data[\"video\"]\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Check a frame ...\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "plt.imshow(inputs[0][0][0].to(\"cpu\"), cmap=\"gray\")\n",
    "plt.plot()\n",
    "\n",
    "inputs = [i.to(device)[None, ...] for i in inputs]\n",
    "crime_detector = crime_detector.to(device)\n",
    "\n",
    "# Slowfast forward propagation\n",
    "outputs = crime_detector(inputs)\n",
    "\n",
    "print(outputs.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference of classifier\n",
    "\n",
    "Using snippets from \"live demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import Compose  # Ensure your transform is defined\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "#####################################################################\n",
    "# REMINDER :\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "frame_delay = int(1000 / 60)  # Delay per frame in milliseconds\n",
    "slowfast_alpha = 4\n",
    "num_clips = 10\n",
    "num_crops = 3\n",
    "clip_duration = (num_frames * sampling_rate) / frames_per_second\n",
    "#####################################################################\n",
    "\n",
    "dcsass_path = \"./DCSASS/Shoplifting\"\n",
    "\n",
    "# Resize scale factor\n",
    "scale_factor = 2.0  # Change this to control the scaling (e.g., 1.5, 2.0, etc.)\n",
    "\n",
    "# Iterate through directories and files\n",
    "for folder in os.listdir(dcsass_path):\n",
    "    folder_path = os.path.join(dcsass_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            video_path = os.path.join(folder_path, file)\n",
    "\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "            if not cap.isOpened():\n",
    "                raise Exception(f\"Error: Could not open video file {video_path}\")\n",
    "\n",
    "            # Compute video score only once for the video snippet\n",
    "            video = EncodedVideo.from_path(video_path)\n",
    "            video_data = video.get_clip(start_sec=0, end_sec=clip_duration)\n",
    "            video_data = transform(video_data)\n",
    "            inputs = video_data[\"video\"]\n",
    "\n",
    "            inputs = [i.to(device)[None, ...] for i in inputs]\n",
    "            crime_detector = crime_detector.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = crime_detector(inputs)\n",
    "            raw_score = float(preds[0].item())\n",
    "            score = 100*torch.sigmoid(torch.tensor(3*(raw_score+1))).item()\n",
    "            print(f\"Prediction score for {file}: {score:.2f}%\")\n",
    "\n",
    "            # Display the video with text overlay\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # resize\n",
    "                original_height, original_width = frame.shape[:2]\n",
    "                new_width = int(original_width * scale_factor)\n",
    "                new_height = int(original_height * scale_factor)\n",
    "                frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "                # add overlay for da post to pop\n",
    "                overlay = frame.copy()\n",
    "                alpha = 0.6  # Transparency factor\n",
    "                cv2.rectangle(overlay, (5, int(new_height * 0.05)), \n",
    "                            (new_width - 5, int(new_height * 0.2)), (0, 0, 0), -1)\n",
    "                frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "                # dips text\n",
    "                text = f\"Crime Score: {score:.2f} %\"\n",
    "                color = (0, 0, 255) if score > 50 else (0, 255, 0)\n",
    "                cv2.putText(frame, text, (20, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "                cv2.imshow('Crime Detection', frame)\n",
    "                if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "# Close all windows at the end\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
