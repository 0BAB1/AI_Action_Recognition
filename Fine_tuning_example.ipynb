{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Action recognition course : LAB 2\n",
        "\n",
        "> Author : BABIN-RIBY Hugo, See LICENSE FIle\n",
        "\n",
        "This lab is meant to be completed by students. There is a set of challenges they need to overcome by asking questions to gain experience.\n",
        "\n",
        "> Note that this notebook was meant to be executed in google colab but can easily be ported to other services with minimal work.\n",
        "\n",
        "At the end of this notebook (~1h30-2h) you will :\n",
        "\n",
        "- Have a better expertise in video data inference\n",
        "- Have hands-on experience with\n",
        "  - action recognition models\n",
        "  - action recognition datasets\n",
        "- Know the pros and cons of each type of architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VfENHEcgoC0q",
        "outputId": "bee7ec31-285c-47b4-8967-7a9d114ae7d4"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle;\n",
        "!mkdir -p ~/.kaggle;\n",
        "!cp kaggle.json ~/.kaggle/;\n",
        "!chmod 600 ~/.kaggle/kaggle.json;\n",
        "!kaggle datasets download -d mateohervas/dcsass-dataset -p /content/;\n",
        "!unzip /content/dcsass-dataset.zip -d /content/;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mldf-7H5pFId",
        "outputId": "7a98ecae-e3d2-4014-b5cc-aacab73bca61"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install pytorchvideo\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyq4szPFpIpH"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BuMLng7FpufW",
        "outputId": "20ca5b00-4ca8-4494-f167-b14e9e87951f"
      },
      "outputs": [],
      "source": [
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "num_clips = 10\n",
        "num_crops = 3\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "U1GoJ2-kpzYp",
        "outputId": "be959da2-c573-45b1-9f85-5dfa4ace3e2e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(device)\n",
        "\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "!ls\n",
        "\n",
        "video = EncodedVideo.from_path(\"./DCSASS Dataset/Shoplifting/Shoplifting001_x264.mp4/Shoplifting001_x264_19.mp4\")\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "video_data = transform(video_data)\n",
        "inputs = video_data[\"video\"]\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "# Check a frame ...\n",
        "print(inputs[0].shape)\n",
        "print(inputs[1].shape)\n",
        "plt.imshow(inputs[0][0][0].to(\"cpu\"), cmap=\"gray\")\n",
        "plt.plot()\n",
        "\n",
        "inputs = [i.to(device)[None, ...] for i in inputs]\n",
        "model = model.to(device)\n",
        "\n",
        "# Slowfast forward propagation\n",
        "outputs = model(inputs)\n",
        "\n",
        "print(outputs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhpn55VwqAsD"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, num_classes=2):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N16ven3ersmU",
        "outputId": "f52b97f6-0f76-4a9b-b7c2-0d0ebd41e8e3"
      },
      "outputs": [],
      "source": [
        "num_classes = 2 # [prob shoplifting, prob NOT shoplifting]\n",
        "slowfast_out = model.blocks[-1].proj.out_features\n",
        "print(slowfast_out)\n",
        "\n",
        "fine_tune = CustomClassifier(slowfast_out, [512, 128], num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7PXn7jHrwM8",
        "outputId": "8c2d3009-57d0-4c39-8af4-276dc09f30ea"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.share\n",
        "\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_folder, label_csv, transform=None, clip_duration=2):\n",
        "        self.video_folder = video_folder\n",
        "        self.data = pd.read_csv(label_csv, header=None, names=['Name', 'Type', 'Label'])\n",
        "        self.data = self.data.drop(columns=['Type'])\n",
        "        self.transform = transform\n",
        "        self.clip_duration = clip_duration  # Duration of video clip in seconds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get video path and label\n",
        "        row = self.data.iloc[idx]\n",
        "        video_path = os.path.join(self.video_folder, row['Name'][0:19] + \".mp4/\" + row['Name'] + \".mp4\")\n",
        "        label = row['Label']\n",
        "        if label :\n",
        "            label = torch.tensor([1,0])\n",
        "        else :\n",
        "            label = torch.tensor([0,1])\n",
        "\n",
        "        # Load video\n",
        "        video = EncodedVideo.from_path(video_path)\n",
        "        # Sample clip (start at 0 for simplicity)\n",
        "        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "        video_data = transform(video_data)['video']\n",
        "        return video_data, label\n",
        "\n",
        "dataset = VideoDataset(\"./DCSASS Dataset/Shoplifting\", \"./DCSASS Dataset/Labels/Shoplifting.csv\", transform, clip_duration)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7N6D2FeryFp"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "device = \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fine_tune = fine_tune.to(device)\n",
        "model=model.to(device)\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = Adam(fine_tune.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wPsK1B2Ur4Q0",
        "outputId": "eb63f81b-6fb1-436a-fdae-d7f03abae7da"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize a list to store the running loss values\n",
        "epoch_losses = []\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.eval()\n",
        "    fine_tune.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        i += 1\n",
        "        slow_pathway = inputs[0].to(device)\n",
        "        fast_pathway = inputs[1].to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model([slow_pathway, fast_pathway])\n",
        "\n",
        "        classification = fine_tune(outputs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(classification, torch.reshape(labels,classification.shape).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        print(\"Epoch #\",epoch, \" // Progress : \",i/len(train_loader), \"% // Loss :\", loss.item())\n",
        "\n",
        "    # Calculate the average loss for this epoch and store it\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "# Plot the running loss after the training loop\n",
        "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD7ruqYhvHOH",
        "outputId": "97201ed0-40f0-41f7-f16a-1f80fb1c97e9"
      },
      "outputs": [],
      "source": [
        "# torch.save(fine_tune.state_dict(), 'fine_tune_model.pth')\n",
        "# fine_tune.load_state_dict(torch.load('fine_tune_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnGIbG2v_aga",
        "outputId": "911ea96b-0aec-490d-b750-067295398eb4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device=\"cpu\"\n",
        "\n",
        "model.to(device)\n",
        "fine_tune.to(device)\n",
        "\n",
        "model.eval()\n",
        "fine_tune.eval()\n",
        "\n",
        "num_tests = 100\n",
        "correct_positive = 0\n",
        "correct_negative = 0\n",
        "test = 0\n",
        "false_positive = 0\n",
        "false_negative = 0\n",
        "\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "    slow_pathway = inputs[0].to(device)\n",
        "    fast_pathway = inputs[1].to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model([slow_pathway, fast_pathway])\n",
        "    classification = fine_tune(outputs)\n",
        "\n",
        "    shoplifting_label = bool(labels[0][0])\n",
        "    classification_label = bool(classification[0][0] > -1) # treshold technique\n",
        "\n",
        "    correct_positive += shoplifting_label and classification_label\n",
        "    correct_negative += not shoplifting_label and not classification_label\n",
        "    false_positive += not shoplifting_label and classification_label\n",
        "    false_negative += shoplifting_label and not classification_label\n",
        "\n",
        "    print(test+1,shoplifting_label, classification_label, \" // ACCURACY TRACKING : \", correct_positive, correct_negative, false_positive, false_negative)\n",
        "\n",
        "    test += 1\n",
        "    if test == 99:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J61xMMLLAhSh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
